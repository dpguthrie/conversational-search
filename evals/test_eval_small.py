"""Test evaluation with a small subset of data using stateful conversations."""

import json
from typing import Any, Dict, List

from braintrust import init_experiment
from dotenv import load_dotenv

from evals.scorers.answer_completeness import answer_completeness_scorer
from evals.scorers.citation_quality import citation_quality_scorer
from evals.scorers.factual_accuracy import factual_accuracy_scorer
from evals.scorers.retrieval_quality import retrieval_quality_scorer
from src.agent import ConversationalSearchAgent

# Load environment variables
load_dotenv()


def load_test_conversations_small() -> List[Dict[str, Any]]:
    """Load first conversation for quick testing."""
    with open("evals/test_data.json", "r") as f:
        conversations = json.load(f)

    # Take only first conversation
    return [conversations[0]]


def run_scorers(output: Dict[str, Any], parent_span: Any = None) -> Dict[str, float]:
    """Run all scorers on an output with tracing.

    Args:
        output: Agent output dict with answer and sources
        parent_span: Parent span to attach scorer spans to

    Returns:
        Dict of scorer name to score value
    """
    scores = {}

    # Run each scorer with its own span for visibility
    scorers_list = [
        ("factual_accuracy", factual_accuracy_scorer),
        ("citation_quality", citation_quality_scorer),
        ("answer_completeness", answer_completeness_scorer),
        ("retrieval_quality", retrieval_quality_scorer),
    ]

    for scorer_name, scorer in scorers_list:
        try:
            # Create span for scorer execution
            if parent_span:
                with parent_span.start_span(
                    name=f"scorer_{scorer_name}", span_attributes={"type": "score"}
                ) as scorer_span:
                    scorer_span.log(input={"output": output})
                    result = scorer(output)

                    # Handle both Score objects and dict returns
                    if isinstance(result, dict):
                        score_value = result["score"]
                        score_key = result["name"]
                        scorer_span.log(
                            output=result, scores={score_key: score_value}
                        )
                    else:
                        score_value = result.score
                        score_key = result.name
                        scorer_span.log(output=result, scores={score_key: score_value})

                    scores[score_key] = score_value
            else:
                # Fallback if no parent span
                result = scorer(output)
                if isinstance(result, dict):
                    scores[result["name"]] = result["score"]
                else:
                    scores[result.name] = result.score

        except Exception as e:
            print(f"Warning: Scorer {scorer_name} failed: {e}")
            scores[scorer_name] = 0.0

    return scores


def evaluate_conversation(
    conversation: Dict[str, Any], experiment: Any
) -> List[Dict[str, Any]]:
    """Evaluate a single conversation with stateful context.

    Args:
        conversation: Conversation dict with topic and turns
        experiment: Braintrust experiment object

    Returns:
        List of results for each turn
    """
    agent = ConversationalSearchAgent()
    thread_id = None  # Start with no thread_id
    results = []

    test_thread_id = conversation["thread_id"]  # From test data (semantic ID)
    topic = conversation["topic"]

    for turn_idx, turn in enumerate(conversation["turns"]):
        query = turn["query"]
        turn_type = turn.get("type", "unknown")

        # Start a span for this turn (since agent uses @traced)
        with experiment.start_span(
            name=f"turn_{turn_idx}",
            input={"query": query, "turn_type": turn_type},
        ) as span:
            # Run agent with thread_id to maintain conversation context
            response, agent_thread_id = agent.run(query, thread_id=thread_id)
            thread_id = agent_thread_id  # Update for next turn

            # Get final state to extract sources
            state = agent.get_state(agent_thread_id)

            output = {
                "query": query,
                "response": response,
                "sources": state.get("sources", []),
                "thread_id": agent_thread_id,
            }

            # Run scorers with tracing (pass span for scorer sub-spans)
            scores = run_scorers(output, parent_span=span)

            # Log to the span
            span.log(
                output=output,
                scores=scores,
                metadata={
                    "test_thread_id": test_thread_id,  # From test data
                    "agent_thread_id": agent_thread_id,  # Generated by agent
                    "topic": topic,
                    "turn_index": turn_idx,
                    "turn_type": turn_type,
                },
            )

        results.append(
            {
                "query": query,
                "output": output,
                "scores": scores,
                "turn_type": turn_type,
            }
        )

        # Print progress
        print(
            f"  Turn {turn_idx + 1}/{len(conversation['turns'])} ({turn_type}): {query[:60]}..."
        )

    return results


def main():
    """Run small evaluation test with stateful conversations."""
    print("=" * 60)
    print("SMALL EVAL TEST - Stateful Conversation")
    print("=" * 60)
    print()

    print("Loading test data...")
    conversations = load_test_conversations_small()
    total_turns = sum(len(conv["turns"]) for conv in conversations)
    print(f"Loaded {len(conversations)} conversation ({total_turns} turns)")
    print()

    print("Initializing Braintrust experiment...")
    experiment = init_experiment(
        project="conversational-search",
        experiment="test_small_stateful",
        metadata={
            "model": "gpt-4o-mini",
            "tavily_depth": "advanced",
            "evaluation_type": "stateful_conversations",
            "description": "Small test with first conversation",
        },
    )
    print(f"Experiment initialized: {experiment.id}")
    print()

    print("Running evaluation...")
    print(
        "Scorers: factual_accuracy, citation_quality, answer_completeness, retrieval_quality"
    )
    print()

    all_results = []
    for conv_idx, conversation in enumerate(conversations, 1):
        print(
            f"Conversation {conv_idx}/{len(conversations)}: {conversation['topic']}"
        )
        results = evaluate_conversation(conversation, experiment)
        all_results.extend(results)
        print()

    # Finish experiment
    try:
        experiment.flush()
    except AttributeError:
        pass  # flush may not be available on all experiment types

    print("=" * 60)
    print("âœ“ EVALUATION COMPLETE")
    print("=" * 60)
    print(f"Evaluated {len(conversations)} conversation")
    print(f"Total turns evaluated: {len(all_results)}")
    print()

    # Calculate average scores
    score_names = ["factual_accuracy", "citation_quality", "answer_completeness", "retrieval_quality"]
    print("Average Scores:")
    for score_name in score_names:
        scores = [r["scores"].get(score_name, 0.0) for r in all_results]
        avg = sum(scores) / len(scores) if scores else 0.0
        print(f"  {score_name}: {avg:.4f}")
    print()

    print("View in Braintrust dashboard:")
    print(f"https://www.braintrust.dev/app/Braintrust%20Demos/p/conversational-search/experiments/{experiment.id}")
    print("=" * 60)


if __name__ == "__main__":
    main()
